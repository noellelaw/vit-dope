{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j8-PwQL_7Iy",
        "outputId": "7532ee0a-d80f-4db9-b234-8b61ea51e628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_Dvd85zk-zk6"
      },
      "outputs": [],
      "source": [
        "#@title Configure kaggle and dowload dome-mesh-ycb dataset (only once, takes ~34 min)\n",
        "import os\n",
        "# Must download kaggle environmnet key and place in folder of choice\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/DeepLearning/Kaggle\"\n",
        "%cd /content/drive/MyDrive/DeepLearning/Kaggle/\n",
        "#!kaggle datasets download -d noellelaw/dome-mesh-ycb --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Tw9S7L4BBz",
        "outputId": "a11270e2-4803-4517-bc11-9e87e3592688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
            "Collecting mmcv-full==v1.3.9\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv_full-1.3.9-cp38-cp38-manylinux1_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (1.21.6)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (7.1.2)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (4.6.0.66)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.3.9 yapf-0.32.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install needed repositories\n",
        "!pip install mmcv-full==v1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZABr3qNbUvl-",
        "outputId": "cce59b20-a13d-46b8-eec2-25525c5a5137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'vit-dope'...\n",
            "remote: Enumerating objects: 156, done.\u001b[K\n",
            "remote: Counting objects: 100% (156/156), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 156 (delta 71), reused 109 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (156/156), 655.14 KiB | 2.66 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/vit-dope\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chumpy\n",
            "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting json_tricks\n",
            "  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Collecting munkres\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.14.0+cu116)\n",
            "Collecting xtcocotools>=1.8\n",
            "  Downloading xtcocotools-1.12-cp38-cp38-manylinux1_x86_64.whl (314 kB)\n",
            "\u001b[K     |████████████████████████████████| 314 kB 17.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (0.29.32)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2022.12.7)\n",
            "Building wheels for collected packages: chumpy\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58286 sha256=d719c9994ed689428d454a746fb2d4c6573814289cc11791657ab26fa177eb97\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/a2/b8/b8aeeeaeb01b5002085156add1aed832f2fb03e79d0f22dfed\n",
            "Successfully built chumpy\n",
            "Installing collected packages: xtcocotools, munkres, json-tricks, dataclasses, chumpy\n",
            "Successfully installed chumpy-0.70 dataclasses-0.6 json-tricks-3.16.1 munkres-1.1.4 xtcocotools-1.12\n"
          ]
        }
      ],
      "source": [
        "#@title Clone ViTDope\n",
        "%cd /content/\n",
        "! git clone https://github.com/noellelaw/vit-dope\n",
        "%cd /content/vit-dope\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FdQeBMv6fU1",
        "outputId": "57140578-36ff-42c6-d6ab-494698a1acc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.4.9\n",
            "  Downloading timm-0.4.9-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 611 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.4.9) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from timm==0.4.9) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->timm==0.4.9) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (2022.12.7)\n",
            "Installing collected packages: timm, einops\n",
            "Successfully installed einops-0.6.0 timm-0.4.9\n"
          ]
        }
      ],
      "source": [
        "#@title Install timm and einops\n",
        "! pip install timm==0.4.9 einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwG2ZTR-IIjm"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import copy\n",
        "import os\n",
        "import os.path as osp\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime\n",
        "import glob\n",
        "import cv2\n",
        "import colorsys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "from torch.distributions import MultivariateNormal as MVN\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "from math import acos\n",
        "from math import sqrt\n",
        "from math import pi  \n",
        "\n",
        "import mmcv\n",
        "from mmcv import Config, DictAction\n",
        "from mmcv.utils import get_git_hash\n",
        "from mmcv.runner import get_dist_info, init_dist, set_random_seed\n",
        "\n",
        "from collections import OrderedDict\n",
        "import tempfile\n",
        "import random\n",
        "from __future__ import print_function\n",
        "\n",
        "from models.backbones import ViT\n",
        "from scripts.ndds_dataloader import MultipleVertexJson\n",
        "from core.evaluation.top_down_eval import (keypoint_pck_accuracy,\n",
        "                            keypoints_from_heatmaps,\n",
        "                            pose_pck_accuracy)\n",
        "from models.heads import TopdownHeatmapSimpleHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s27ukYHw3-oh"
      },
      "outputs": [],
      "source": [
        "#@title Training hyperparameters\n",
        "YCB_OBJECT = 'cracker_box'#@param{type:'string'} \n",
        "# Path to training data\n",
        "DATA_PATH = '/content/drive/MyDrive/DeepLearning/Kaggle/cracker_box' #@param{type:'string'} \n",
        "# Path to testing data\n",
        "DATA_PATH_TEST = ''#@param{type:'string'} \n",
        "# Path to pretrained MAE vit-b\n",
        "PRETRAINED = '/content/drive/MyDrive/DeepLearning/mae_pretrain_vit_base.pth'#@param{type:'string'} \n",
        "# Path to weights to resum training from\n",
        "FROM_NET = '/content/drive/MyDrive/DeepLearning/cracker_box_train/net_epoch_62.pth'#@param{type:'string'} \n",
        "# Path to output weight and loss data\n",
        "OUT_FLDR = '/content/drive/MyDrive/DeepLearning/cracker_box_train'#@param{type:'string'} \n",
        "# What you want to name weight files\n",
        "NAME_FILE = 'epoch'#@param{type:'string'} \n",
        "LEARNING_RATE = 5e-4#@param{type:'number'}\n",
        "EPOCHS = 60#@param{type:'integer'}\n",
        "# Tunable parameter for BMSE loss function\n",
        "BMSE_NOISE = 0.1#@param{type:'number'}\n",
        "BATCH_SIZE = 64#@param{type:'integer'}\n",
        "IMAGE_SIZE = 256#@param{type:'integer'}\n",
        "FREEZE_BACKBONE = False#@param{type:'boolean'}\n",
        "DATASIZE = None\n",
        "SAVE = False\n",
        "NORMAL_IMGS = None\n",
        "MAX_NORM = 1.\n",
        "NORM_TYPE = 2\n",
        "NUM_BELIEFS = 9\n",
        "NUM_AFFINITIES = 16\n",
        "NOISE = 1e-5\n",
        "BRIGHTNESS = 1e-5\n",
        "CONTRAST = 1e-5\n",
        "LOG_INTERVAL = 3\n",
        "SIGMA = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DXjBvZmx_XEU"
      },
      "outputs": [],
      "source": [
        "#@title Empty cuda cache as needed\n",
        "# GPU messin with my workflow \n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evqhqDNj0ZqD",
        "outputId": "9f08fe3e-b573-40f4-ea01-f3092a78150b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        }
      ],
      "source": [
        "#@title Get training and testing data loaders \n",
        "print (\"Loading data...\")\n",
        "# Image transform\n",
        "transform = transforms.Compose([\n",
        "                          transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                          transforms.ToTensor()])\n",
        "# Get training data loader\n",
        "trainingdata = None\n",
        "if not DATA_PATH == \"\":\n",
        "  train_dataset = MultipleVertexJson(\n",
        "      root=DATA_PATH,\n",
        "      objectofinterest=YCB_OBJECT,\n",
        "      keep_orientation = True,\n",
        "      noise = NOISE,\n",
        "      sigma = SIGMA,\n",
        "      data_size = DATASIZE,\n",
        "      save = SAVE,\n",
        "      transform = transform,\n",
        "      normal = NORMAL_IMGS,\n",
        "      target_transform = transforms.Compose([\n",
        "                              transforms.Resize(IMAGE_SIZE//4),\n",
        "          ]),\n",
        "      )\n",
        "  trainingdata = torch.utils.data.DataLoader(train_dataset,\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True\n",
        "      )\n",
        "\n",
        "# Get testing data loader\n",
        "testingdata = None\n",
        "if not DATA_PATH_TEST == \"\":\n",
        "  testingdata = torch.utils.data.DataLoader(\n",
        "      MultipleVertexJson(\n",
        "          root = DATA_PATH_TEST,\n",
        "          objectofinterest=YCB_OBJECT,\n",
        "          keep_orientation = True,\n",
        "          noise = NOISE,\n",
        "          sigma = SIGMA,\n",
        "          data_size = DATASIZE,\n",
        "          save = SAVE,\n",
        "          transform = transform,\n",
        "          normal = NORMAL_IMGS,\n",
        "          target_transform = transforms.Compose([\n",
        "                                  transforms.Resize(IMAGE_SIZE//4),\n",
        "              ]),\n",
        "          ),\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naSdKSkVmbRD"
      },
      "outputs": [],
      "source": [
        "#@title Set up ViTDope Network\n",
        "class ViTDopeNetwork(nn.Module):\n",
        "  def __init__(\n",
        "            self,\n",
        "            pretrained=False,\n",
        "            numBeliefMap=9,\n",
        "            numAffinity=16\n",
        "            ):\n",
        "    super(ViTDopeNetwork, self).__init__()\n",
        "    # Set up backbone accordance with ViT-B\n",
        "    backbone = ViT(img_size=(256,256),\n",
        "                  patch_size=16,\n",
        "                  embed_dim=768,\n",
        "                  depth=12,\n",
        "                  num_heads=12,\n",
        "                  ratio=1,\n",
        "                  use_checkpoint=False,\n",
        "                  mlp_ratio=4,\n",
        "                  qkv_bias=True,\n",
        "                  drop_path_rate=0.3,\n",
        "    )\n",
        "    # Init ViT weights from ViT MAE trained on image net\n",
        "    if not PRETRAINED == '':\n",
        "        backbone.init_weights(pretrained=PRETRAINED)\n",
        "    # Set classical decoder head for belief maps\n",
        "    belief_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numBeliefMap,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "    # Set classical decoder head for affity maps\n",
        "    affinity_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numAffinity,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "\n",
        "    self.backbone = nn.Sequential(*[backbone])\n",
        "    self.belief_head = nn.Sequential(*[belief_head])\n",
        "    self.affinity_head = nn.Sequential(*[affinity_head])\n",
        "\n",
        "  # Forward\n",
        "  def forward(self, x):\n",
        "    backbone_out = self.backbone(x)\n",
        "    belief_out = self.belief_head(backbone_out)\n",
        "    affinity_out = self.affinity_head(backbone_out)\n",
        "    return belief_out, affinity_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF_M3-c7tkSk"
      },
      "outputs": [],
      "source": [
        "#@title Set up files for testing & training progress\n",
        "with open (OUT_FLDR+'/loss_train.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')\n",
        "\n",
        "with open (OUT_FLDR+'/loss_test.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ySOTsi3mepE",
        "outputId": "c98f6ab4-e368-4c0b-9e42-f18f5b3274fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from prior weights...\n"
          ]
        }
      ],
      "source": [
        "#@title Load model\n",
        "net = ViTDopeNetwork()\n",
        "net = net.to('cuda')\n",
        "# Load for inference or to resume training\n",
        "if not FROM_NET == '':\n",
        "    print('Loading model from prior weights...')\n",
        "    net.load_state_dict(torch.load(FROM_NET))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOj3did6ayGV"
      },
      "outputs": [],
      "source": [
        "#@title Set up optimizer and scheduler\n",
        "if FREEZE_BACKBONE:\n",
        "  for name, param in net.named_parameters():            \n",
        "      if name.startswith('backbone'):\n",
        "          param.requires_grad = False\n",
        "\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "optimizer = torch.optim.AdamW(parameters,\n",
        "                              lr=LEARNING_RATE, \n",
        "                              betas=(0.9, 0.999), \n",
        "                              weight_decay=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcjW9rhn-_rq",
        "outputId": "b1c2fcaf-9a51-4ac1-f5ab-c7c18b35aa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters:  94241049\n"
          ]
        }
      ],
      "source": [
        "#@title Print out model parameters\n",
        "count = 0\n",
        "for p in net.parameters():\n",
        "    if p.requires_grad:\n",
        "      count += p.numel() \n",
        "    \n",
        "print(\"Number of trainable parameters: \", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qLcjNT-HX_E"
      },
      "outputs": [],
      "source": [
        "#@title Get balanced MSE Loss\n",
        "# adapted from: https://github.com/jiawei-ren/BalancedMSE\n",
        "def get_bmse_loss(preds, targets):\n",
        "    # Batch size, num outputs, height, width\n",
        "    B,N,H,W = preds.shape \n",
        "    resize_to = H*W\n",
        "    loss = 0\n",
        "    for i in range(N):   \n",
        "        # Get identity matrix      \n",
        "        I = torch.eye( resize_to )\n",
        "        # Reshape target and belief maps\n",
        "        belief = preds[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        target = targets[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        # Use trainign distribution prior to make statistical conversion for mse\n",
        "        # logit size: [batch, batch]\n",
        "        logits = MVN(belief.unsqueeze(1), (BMSE_NOISE*I)).log_prob(target.unsqueeze(0))  \n",
        "        # Apply contrastive-like loss\n",
        "        loss_temp = cross_entropy(logits, torch.arange(B))     \n",
        "        loss_temp = loss_temp * (2 * BMSE_NOISE)\n",
        "        loss += loss_temp\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2CMHoaPXt_vM"
      },
      "outputs": [],
      "source": [
        "#@title Run the network for one epoch \n",
        "def _run_network(epoch, loader, train=True):\n",
        "\n",
        "    if train:\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, targets in enumerate(loader):\n",
        "        # Get data and targets\n",
        "        data = Variable(targets['img'].cuda())\n",
        "        target_belief = Variable(targets['beliefs'].cuda())        \n",
        "        target_affinity = Variable(targets['affinities'].cuda())\n",
        "        loss = None\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Get predictions\n",
        "        output_belief, output_affinities = net(data) \n",
        "\n",
        "        # Get balanced mse loss for belief maps\n",
        "        loss = get_bmse_loss(output_belief, target_belief)\n",
        "\n",
        "        # Get balanced mse loss for affinity maps\n",
        "        loss += get_bmse_loss(output_affinities, target_affinity)\n",
        "\n",
        "        # Update weights\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(parameters, max_norm=MAX_NORM, norm_type=NORM_TYPE)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Determine file to write loss into \n",
        "        if train:\n",
        "            namefile = '/loss_train.csv'\n",
        "        else:\n",
        "            namefile = '/loss_test.csv'\n",
        "        # Write to files\n",
        "        with open (OUT_FLDR+namefile,'a') as file:\n",
        "            s = '{}, {},{:.15f}\\n'.format(\n",
        "                epoch,batch_idx,loss.data.item()) \n",
        "            file.write(s)\n",
        "\n",
        "        # Print results\n",
        "        if train:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n",
        "        else:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSl6eqdbvTFN"
      },
      "outputs": [],
      "source": [
        "#@title Run training over all epochs \n",
        "\n",
        "print (\"Start:\" , datetime.datetime.now().time())\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    if not trainingdata is None:\n",
        "        _run_network(epoch,trainingdata)\n",
        "\n",
        "    if not DATA_PATH_TEST == \"\":\n",
        "        _run_network(epoch,testingdata,train = False)\n",
        "        if  DATA_PATH == \"\":\n",
        "            break # lets get out of this if we are only testing\n",
        "    try:\n",
        "        torch.save(net.state_dict(), '{}/net_{}_{}.pth'.format(OUT_FLDR, NAME_FILE, epoch))\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "print (\"End:\" , datetime.datetime.now().time())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}