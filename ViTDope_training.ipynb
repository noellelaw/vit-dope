{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j8-PwQL_7Iy",
        "outputId": "7532ee0a-d80f-4db9-b234-8b61ea51e628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_Dvd85zk-zk6"
      },
      "outputs": [],
      "source": [
        "#@title Configure kaggle and dowload dome-mesh-ycb dataset (only once, takes ~34 min)\n",
        "import os\n",
        "# Must download kaggle environmnet key and place in folder of choice\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/DeepLearning/Kaggle\"\n",
        "%cd /content/drive/MyDrive/DeepLearning/Kaggle/\n",
        "#!kaggle datasets download -d noellelaw/dome-mesh-ycb --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "00Tw9S7L4BBz"
      },
      "outputs": [],
      "source": [
        "#@title Install needed repositories\n",
        "!pip install mmcv-full==v1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZABr3qNbUvl-"
      },
      "outputs": [],
      "source": [
        "#@title Clone ViTDope\n",
        "%cd /content/\n",
        "! git clone https://github.com/noellelaw/vit-dope\n",
        "%cd /content/vit-dope\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FdQeBMv6fU1"
      },
      "outputs": [],
      "source": [
        "#@title Install timm and einops\n",
        "! pip install timm==0.4.9 einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwG2ZTR-IIjm"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import copy\n",
        "import os\n",
        "import os.path as osp\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime\n",
        "import glob\n",
        "import cv2\n",
        "import colorsys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "from torch.distributions import MultivariateNormal as MVN\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "from math import acos\n",
        "from math import sqrt\n",
        "from math import pi  \n",
        "\n",
        "import mmcv\n",
        "from mmcv import Config, DictAction\n",
        "from mmcv.utils import get_git_hash\n",
        "from mmcv.runner import get_dist_info, init_dist, set_random_seed\n",
        "\n",
        "from collections import OrderedDict\n",
        "import tempfile\n",
        "import random\n",
        "from __future__ import print_function\n",
        "\n",
        "from models.backbones import ViT\n",
        "from scripts.ndds_dataloader import MultipleVertexJson\n",
        "from core.evaluation.top_down_eval import (keypoint_pck_accuracy,\n",
        "                            keypoints_from_heatmaps,\n",
        "                            pose_pck_accuracy)\n",
        "from models.heads import TopdownHeatmapSimpleHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s27ukYHw3-oh"
      },
      "outputs": [],
      "source": [
        "#@title Training hyperparameters\n",
        "YCB_OBJECT = 'cracker_box'#@param{type:'string'} \n",
        "# Path to training data\n",
        "DATA_PATH = '/content/drive/MyDrive/DeepLearning/Kaggle/cracker_box' #@param{type:'string'} \n",
        "# Path to testing data\n",
        "DATA_PATH_TEST = ''#@param{type:'string'} \n",
        "# Path to pretrained MAE vit-b\n",
        "PRETRAINED = '/content/drive/MyDrive/DeepLearning/mae_pretrain_vit_base.pth'#@param{type:'string'} \n",
        "# Path to weights to resum training from\n",
        "FROM_NET = '/content/drive/MyDrive/DeepLearning/cracker_box_train/net_epoch_62.pth'#@param{type:'string'} \n",
        "# Path to output weight and loss data\n",
        "OUT_FLDR = '/content/drive/MyDrive/DeepLearning/cracker_box_train'#@param{type:'string'} \n",
        "# What you want to name weight files\n",
        "NAME_FILE = 'epoch'#@param{type:'string'} \n",
        "LEARNING_RATE = 5e-4#@param{type:'number'}\n",
        "EPOCHS = 60#@param{type:'integer'}\n",
        "# Tunable parameter for BMSE loss function\n",
        "BMSE_NOISE = 0.1#@param{type:'number'}\n",
        "BATCH_SIZE = 64#@param{type:'integer'}\n",
        "IMAGE_SIZE = 256#@param{type:'integer'}\n",
        "FREEZE_BACKBONE = False#@param{type:'boolean'}\n",
        "DATASIZE = None\n",
        "SAVE = False\n",
        "NORMAL_IMGS = None\n",
        "MAX_NORM = 1.\n",
        "NORM_TYPE = 2\n",
        "NUM_BELIEFS = 9\n",
        "NUM_AFFINITIES = 16\n",
        "NOISE = 1e-5\n",
        "BRIGHTNESS = 1e-5\n",
        "CONTRAST = 1e-5\n",
        "LOG_INTERVAL = 3\n",
        "SIGMA = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DXjBvZmx_XEU"
      },
      "outputs": [],
      "source": [
        "#@title Empty cuda cache as needed\n",
        "# GPU messin with my workflow \n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evqhqDNj0ZqD",
        "outputId": "9f08fe3e-b573-40f4-ea01-f3092a78150b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        }
      ],
      "source": [
        "#@title Get training and testing data loaders \n",
        "print (\"Loading data...\")\n",
        "# Image transform\n",
        "transform = transforms.Compose([\n",
        "                          transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                          transforms.ToTensor()])\n",
        "# Get training data loader\n",
        "trainingdata = None\n",
        "if not DATA_PATH == \"\":\n",
        "  train_dataset = MultipleVertexJson(\n",
        "      root=DATA_PATH,\n",
        "      objectofinterest=YCB_OBJECT,\n",
        "      keep_orientation = True,\n",
        "      noise = NOISE,\n",
        "      sigma = SIGMA,\n",
        "      data_size = DATASIZE,\n",
        "      save = SAVE,\n",
        "      transform = transform,\n",
        "      normal = NORMAL_IMGS,\n",
        "      target_transform = transforms.Compose([\n",
        "                              transforms.Resize(IMAGE_SIZE//4),\n",
        "          ]),\n",
        "      )\n",
        "  trainingdata = torch.utils.data.DataLoader(train_dataset,\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True\n",
        "      )\n",
        "\n",
        "# Get testing data loader\n",
        "testingdata = None\n",
        "if not DATA_PATH_TEST == \"\":\n",
        "  testingdata = torch.utils.data.DataLoader(\n",
        "      MultipleVertexJson(\n",
        "          root = DATA_PATH_TEST,\n",
        "          objectofinterest=YCB_OBJECT,\n",
        "          keep_orientation = True,\n",
        "          noise = NOISE,\n",
        "          sigma = SIGMA,\n",
        "          data_size = DATASIZE,\n",
        "          save = SAVE,\n",
        "          transform = transform,\n",
        "          normal = NORMAL_IMGS,\n",
        "          target_transform = transforms.Compose([\n",
        "                                  transforms.Resize(IMAGE_SIZE//4),\n",
        "              ]),\n",
        "          ),\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naSdKSkVmbRD"
      },
      "outputs": [],
      "source": [
        "#@title Set up ViTDope Network\n",
        "class ViTDopeNetwork(nn.Module):\n",
        "  def __init__(\n",
        "            self,\n",
        "            pretrained=False,\n",
        "            numBeliefMap=9,\n",
        "            numAffinity=16\n",
        "            ):\n",
        "    super(ViTDopeNetwork, self).__init__()\n",
        "    # Set up backbone accordance with ViT-B\n",
        "    backbone = ViT(img_size=(256,256),\n",
        "                  patch_size=16,\n",
        "                  embed_dim=768,\n",
        "                  depth=12,\n",
        "                  num_heads=12,\n",
        "                  ratio=1,\n",
        "                  use_checkpoint=False,\n",
        "                  mlp_ratio=4,\n",
        "                  qkv_bias=True,\n",
        "                  drop_path_rate=0.3,\n",
        "    )\n",
        "    # Init ViT weights from ViT MAE trained on image net\n",
        "    if not PRETRAINED == '':\n",
        "        backbone.init_weights(pretrained=PRETRAINED)\n",
        "    # Set classical decoder head for belief maps\n",
        "    belief_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numBeliefMap,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "    # Set classical decoder head for affity maps\n",
        "    affinity_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numAffinity,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "\n",
        "    self.backbone = nn.Sequential(*[backbone])\n",
        "    self.belief_head = nn.Sequential(*[belief_head])\n",
        "    self.affinity_head = nn.Sequential(*[affinity_head])\n",
        "\n",
        "  # Forward\n",
        "  def forward(self, x):\n",
        "    backbone_out = self.backbone(x)\n",
        "    belief_out = self.belief_head(backbone_out)\n",
        "    affinity_out = self.affinity_head(backbone_out)\n",
        "    return belief_out, affinity_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF_M3-c7tkSk"
      },
      "outputs": [],
      "source": [
        "#@title Set up files for testing & training progress\n",
        "with open (OUT_FLDR+'/loss_train.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')\n",
        "\n",
        "with open (OUT_FLDR+'/loss_test.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ySOTsi3mepE",
        "outputId": "c98f6ab4-e368-4c0b-9e42-f18f5b3274fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from prior weights...\n"
          ]
        }
      ],
      "source": [
        "#@title Load model\n",
        "net = ViTDopeNetwork()\n",
        "net = net.to('cuda')\n",
        "# Load for inference or to resume training\n",
        "if not FROM_NET == '':\n",
        "    print('Loading model from prior weights...')\n",
        "    net.load_state_dict(torch.load(FROM_NET))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOj3did6ayGV"
      },
      "outputs": [],
      "source": [
        "#@title Set up optimizer and scheduler\n",
        "if FREEZE_BACKBONE:\n",
        "  for name, param in net.named_parameters():            \n",
        "      if name.startswith('backbone'):\n",
        "          param.requires_grad = False\n",
        "\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "optimizer = torch.optim.AdamW(parameters,\n",
        "                              lr=LEARNING_RATE, \n",
        "                              betas=(0.9, 0.999), \n",
        "                              weight_decay=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcjW9rhn-_rq",
        "outputId": "b1c2fcaf-9a51-4ac1-f5ab-c7c18b35aa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters:  94241049\n"
          ]
        }
      ],
      "source": [
        "#@title Print out model parameters\n",
        "count = 0\n",
        "for p in net.parameters():\n",
        "    if p.requires_grad:\n",
        "      count += p.numel() \n",
        "    \n",
        "print(\"Number of trainable parameters: \", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qLcjNT-HX_E"
      },
      "outputs": [],
      "source": [
        "#@title Get balanced MSE Loss\n",
        "# adapted from: https://github.com/jiawei-ren/BalancedMSE\n",
        "def get_bmse_loss(preds, targets):\n",
        "    # Batch size, num outputs, height, width\n",
        "    B,N,H,W = preds.shape \n",
        "    resize_to = H*W\n",
        "    loss = 0\n",
        "    for i in range(N):   \n",
        "        # Get identity matrix      \n",
        "        I = torch.eye( resize_to )\n",
        "        # Reshape target and belief maps\n",
        "        belief = preds[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        target = targets[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        # Use trainign distribution prior to make statistical conversion for mse\n",
        "        # logit size: [batch, batch]\n",
        "        logits = MVN(belief.unsqueeze(1), (BMSE_NOISE*I)).log_prob(target.unsqueeze(0))  \n",
        "        # Apply contrastive-like loss\n",
        "        loss_temp = cross_entropy(logits, torch.arange(B))     \n",
        "        loss_temp = loss_temp * (2 * BMSE_NOISE)\n",
        "        loss += loss_temp\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2CMHoaPXt_vM"
      },
      "outputs": [],
      "source": [
        "#@title Run the network for one epoch \n",
        "def _run_network(epoch, loader, train=True):\n",
        "\n",
        "    if train:\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, targets in enumerate(loader):\n",
        "        # Get data and targets\n",
        "        data = Variable(targets['img'].cuda())\n",
        "        target_belief = Variable(targets['beliefs'].cuda())        \n",
        "        target_affinity = Variable(targets['affinities'].cuda())\n",
        "        loss = None\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Get predictions\n",
        "        output_belief, output_affinities = net(data) \n",
        "\n",
        "        # Get balanced mse loss for belief maps\n",
        "        loss = get_bmse_loss(output_belief, target_belief)\n",
        "\n",
        "        # Get balanced mse loss for affinity maps\n",
        "        loss += get_bmse_loss(output_affinities, target_affinity)\n",
        "\n",
        "        # Update weights\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(parameters, max_norm=MAX_NORM, norm_type=NORM_TYPE)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Determine file to write loss into \n",
        "        if train:\n",
        "            namefile = '/loss_train.csv'\n",
        "        else:\n",
        "            namefile = '/loss_test.csv'\n",
        "        # Write to files\n",
        "        with open (OUT_FLDR+namefile,'a') as file:\n",
        "            s = '{}, {},{:.15f}\\n'.format(\n",
        "                epoch,batch_idx,loss.data.item()) \n",
        "            file.write(s)\n",
        "\n",
        "        # Print results\n",
        "        if train:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n",
        "        else:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSl6eqdbvTFN"
      },
      "outputs": [],
      "source": [
        "#@title Run training over all epochs \n",
        "\n",
        "print (\"Start:\" , datetime.datetime.now().time())\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    if not trainingdata is None:\n",
        "        _run_network(epoch,trainingdata)\n",
        "\n",
        "    if not DATA_PATH_TEST == \"\":\n",
        "        _run_network(epoch,testingdata,train = False)\n",
        "        if  DATA_PATH == \"\":\n",
        "            break # lets get out of this if we are only testing\n",
        "    try:\n",
        "        torch.save(net.state_dict(), '{}/net_{}_{}.pth'.format(OUT_FLDR, NAME_FILE, epoch))\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "print (\"End:\" , datetime.datetime.now().time())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}