{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j8-PwQL_7Iy",
        "outputId": "005608f6-5e96-46ad-d1e3-bf8cad160b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect to google drive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dvd85zk-zk6",
        "outputId": "4689d596-02bf-4885-a38e-0a56a3c1ba25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearning/Kaggle\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /content/drive/MyDrive/DeepLearning/Kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "#@title Configure kaggle and dowload dome-mesh-ycb dataset (only once, takes ~34 min)\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/DeepLearning/Kaggle\"\n",
        "%cd /content/drive/MyDrive/DeepLearning/Kaggle/\n",
        "!kaggle datasets download -d noellelaw/dome-mesh-ycb --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Tw9S7L4BBz",
        "outputId": "c73078a1-f7ba-4433-9da0-5a3c2f6c4300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
            "Collecting mmcv-full==v1.3.9\n",
            "  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv_full-1.3.9-cp38-cp38-manylinux1_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (4.6.0.66)\n",
            "Collecting yapf\n",
            "  Downloading yapf-0.32.0-py2.py3-none-any.whl (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from mmcv-full==v1.3.9) (6.0)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Installing collected packages: yapf, addict, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.3.9 yapf-0.32.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install needed repositories\n",
        "!pip install mmcv-full==v1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZABr3qNbUvl-",
        "outputId": "3e281041-ddb8-4267-ca8c-3405fbd98a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'vit-dope'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 78 (delta 19), reused 64 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n",
            "/content/vit-dope\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chumpy\n",
            "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting json_tricks\n",
            "  Downloading json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Collecting munkres\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (0.14.0+cu116)\n",
            "Collecting xtcocotools>=1.8\n",
            "  Downloading xtcocotools-1.12-cp38-cp38-manylinux1_x86_64.whl (314 kB)\n",
            "\u001b[K     |████████████████████████████████| 314 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (57.4.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.8/dist-packages (from xtcocotools>=1.8->-r requirements.txt (line 10)) (0.29.32)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (1.13.0+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision->-r requirements.txt (line 9)) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r requirements.txt (line 9)) (2022.12.7)\n",
            "Building wheels for collected packages: chumpy\n",
            "  Building wheel for chumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58286 sha256=b386dbc01f9d98866ea9808d847de8db50e264ff71fb8162959fa997b53db168\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/a2/b8/b8aeeeaeb01b5002085156add1aed832f2fb03e79d0f22dfed\n",
            "Successfully built chumpy\n",
            "Installing collected packages: xtcocotools, munkres, json-tricks, dataclasses, chumpy\n",
            "Successfully installed chumpy-0.70 dataclasses-0.6 json-tricks-3.16.1 munkres-1.1.4 xtcocotools-1.12\n"
          ]
        }
      ],
      "source": [
        "#@title Clone ViTDope\n",
        "%cd /content/\n",
        "! git clone https://github.com/noellelaw/vit-dope\n",
        "%cd /content/vit-dope\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FdQeBMv6fU1",
        "outputId": "5d2a540c-649b-4424-eb2f-2408fbae13fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.4.9\n",
            "  Downloading timm-0.4.9-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 28.6 MB/s \n",
            "\u001b[?25hCollecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 627 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm==0.4.9) (0.14.0+cu116)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.8/dist-packages (from timm==0.4.9) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4->timm==0.4.9) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm==0.4.9) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->timm==0.4.9) (2022.12.7)\n",
            "Installing collected packages: timm, einops\n",
            "Successfully installed einops-0.6.0 timm-0.4.9\n"
          ]
        }
      ],
      "source": [
        "#@title Install timm and einops\n",
        "! pip install timm==0.4.9 einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YwG2ZTR-IIjm"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import copy\n",
        "import os\n",
        "import os.path as osp\n",
        "from os.path import exists\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import json\n",
        "import datetime\n",
        "import glob\n",
        "import cv2\n",
        "import colorsys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "from torch.distributions import MultivariateNormal as MVN\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "from PIL import Image\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "from math import acos\n",
        "from math import sqrt\n",
        "from math import pi  \n",
        "\n",
        "import mmcv\n",
        "from mmcv import Config, DictAction\n",
        "from mmcv.utils import get_git_hash\n",
        "from mmcv.runner import get_dist_info, init_dist, set_random_seed\n",
        "\n",
        "from collections import OrderedDict\n",
        "import tempfile\n",
        "import random\n",
        "from __future__ import print_function\n",
        "\n",
        "from models.backbones import ViT\n",
        "from ndds_dataloader import MultipleVertexJson\n",
        "from core.evaluation.top_down_eval import (keypoint_pck_accuracy,\n",
        "                            keypoints_from_heatmaps,\n",
        "                            pose_pck_accuracy)\n",
        "from models.heads import TopdownHeatmapSimpleHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "s27ukYHw3-oh"
      },
      "outputs": [],
      "source": [
        "#@title Training hyperparameters\n",
        "DATA_PATH = '/content/drive/MyDrive/DeepLearning/test_cracker'\n",
        "DATA_PATH_TEST = ''\n",
        "PRETRAINED = '/content/drive/MyDrive/DeepLearning/mae_pretrain_vit_base.pth'\n",
        "FROM_NET = ''\n",
        "YCB_OBJECT = 'cracker_box'\n",
        "NOISE = 1e-5\n",
        "BRIGHTNESS = 1e-5\n",
        "CONTRAST = 1e-5\n",
        "BATCH_SIZE = 2\n",
        "IMAGE_SIZE = 256\n",
        "LEARNING_RATE = 5e-4\n",
        "EPOCHS = 60\n",
        "LOG_INTERVAL = 100\n",
        "SIGMA = 4\n",
        "OUT_FLDR = '/content/drive/MyDrive/DeepLearning/cracker_box_train'\n",
        "DATASIZE = None\n",
        "SAVE = False\n",
        "NORMAL_IMGS = None\n",
        "NB_UPDATES = None\n",
        "NAME_FILE = 'epoch'\n",
        "MAX_NORM = 1.\n",
        "NORM_TYPE = 2\n",
        "NUM_BELIEFS = 9\n",
        "NUM_AFFINITIES = 16\n",
        "BMCE_NOISE = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DXjBvZmx_XEU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Empty cuda cache as needed\n",
        "# GPU messin with my workflow \n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evqhqDNj0ZqD",
        "outputId": "87e6bfeb-5f36-4324-f4c4-ca0820dcb6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n"
          ]
        }
      ],
      "source": [
        "#@title Get training and testing data loaders \n",
        "print (\"Loading data...\")\n",
        "transform = transforms.Compose([\n",
        "                          transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
        "                          transforms.ToTensor()])\n",
        "trainingdata = None\n",
        "if not DATA_PATH == \"\":\n",
        "  train_dataset = MultipleVertexJson(\n",
        "      root=DATA_PATH,\n",
        "      objectofinterest=YCB_OBJECT,\n",
        "      keep_orientation = True,\n",
        "      noise = NOISE,\n",
        "      sigma = SIGMA,\n",
        "      data_size = DATASIZE,\n",
        "      save = SAVE,\n",
        "      transform = transform,\n",
        "      normal = NORMAL_IMGS,\n",
        "      target_transform = transforms.Compose([\n",
        "                              transforms.Resize(IMAGE_SIZE//4),\n",
        "          ]),\n",
        "      )\n",
        "  trainingdata = torch.utils.data.DataLoader(train_dataset,\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True\n",
        "      )\n",
        "\n",
        "\n",
        "testingdata = None\n",
        "if not DATA_PATH_TEST == \"\":\n",
        "  testingdata = torch.utils.data.DataLoader(\n",
        "      MultipleVertexJson(\n",
        "          root = DATA_PATH_TEST,\n",
        "          objectofinterest=YCB_OBJECT,\n",
        "          keep_orientation = True,\n",
        "          noise = NOISE,\n",
        "          sigma = SIGMA,\n",
        "          data_size = DATASIZE,\n",
        "          save = SAVE,\n",
        "          transform = transform,\n",
        "          normal = NORMAL_IMGS,\n",
        "          target_transform = transforms.Compose([\n",
        "                                  transforms.Resize(IMAGE_SIZE//4),\n",
        "              ]),\n",
        "          ),\n",
        "      batch_size = BATCH_SIZE, \n",
        "      shuffle = True,\n",
        "      num_workers = 1, \n",
        "      pin_memory = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oXws378EkRZ",
        "outputId": "1f830c14-c220-448a-8633-53c4690d2771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "#@title Test data loader\n",
        "for batch_idx, targets in enumerate(trainingdata):\n",
        "\n",
        "      data = Variable(targets['img'].cuda())\n",
        "      print(data.shape)\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "naSdKSkVmbRD"
      },
      "outputs": [],
      "source": [
        "#@title Set up ViTDope Network\n",
        "class ViTDopeNetwork(nn.Module):\n",
        "  def __init__(\n",
        "            self,\n",
        "            pretrained=False,\n",
        "            numBeliefMap=9,\n",
        "            numAffinity=16\n",
        "            ):\n",
        "    super(ViTDopeNetwork, self).__init__()\n",
        "    # Set up backbone accordance with ViT-B\n",
        "    backbone = ViT(img_size=(256,256),\n",
        "                  patch_size=16,\n",
        "                  embed_dim=768,\n",
        "                  depth=12,\n",
        "                  num_heads=12,\n",
        "                  ratio=1,\n",
        "                  use_checkpoint=False,\n",
        "                  mlp_ratio=4,\n",
        "                  qkv_bias=True,\n",
        "                  drop_path_rate=0.3,\n",
        "    )\n",
        "    # Init ViT weights from ViT MAE trained on image net\n",
        "    if not PRETRAINED == '':\n",
        "        backbone.init_weights(pretrained=PRETRAINED)\n",
        "    # Set classical decoder head for belief maps\n",
        "    belief_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numBeliefMap,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "    # Set classical decoder head for affity maps\n",
        "    affinity_head = TopdownHeatmapSimpleHead(\n",
        "        in_channels=768,\n",
        "        num_deconv_layers=2,\n",
        "        num_deconv_filters=(256, 256),\n",
        "        num_deconv_kernels=(4, 4),\n",
        "        extra=dict(final_conv_kernel=1, ),\n",
        "        out_channels=numAffinity,\n",
        "        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)\n",
        "    )\n",
        "\n",
        "    self.backbone = nn.Sequential(*[backbone])\n",
        "    self.belief_head = nn.Sequential(*[belief_head])\n",
        "    self.affinity_head = nn.Sequential(*[affinity_head])\n",
        "\n",
        "  # Forward\n",
        "  def forward(self, x):\n",
        "    backbone_out = self.backbone(x)\n",
        "    belief_out = self.belief_head(backbone_out)\n",
        "    affinity_out = self.affinity_head(backbone_out)\n",
        "    return belief_out, affinity_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "XF_M3-c7tkSk"
      },
      "outputs": [],
      "source": [
        "#@title Set up files for testing & training progress\n",
        "with open (OUT_FLDR+'/loss_train.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')\n",
        "\n",
        "with open (OUT_FLDR+'/loss_test.csv','w') as file: \n",
        "    file.write('epoch,batchid,loss\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4ySOTsi3mepE"
      },
      "outputs": [],
      "source": [
        "#@title Set up model and optimizer\n",
        "net = ViTDopeNetwork()\n",
        "net = net.to('cuda')\n",
        "# Load for inference or to resume training\n",
        "if FROM_NET!= '':\n",
        "    net.load_state_dict(torch.load(FROM_NET))\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
        "optimizer = torch.optim.AdamW(parameters,\n",
        "                              lr=LEARNING_RATE, \n",
        "                              betas=(0.9, 0.999), \n",
        "                              weight_decay=0.1)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcjW9rhn-_rq",
        "outputId": "ba988cff-e49d-4950-ac1b-1985b2bb3078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters:  94241049\n"
          ]
        }
      ],
      "source": [
        "#@title Print out model parameters\n",
        "count = 0\n",
        "for p in net.parameters():\n",
        "    count += p.numel() \n",
        "    \n",
        "print(\"Number of trainable parameters: \", count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get balanced MCE Loss\n",
        "def get_bmce_loss(preds, targets):\n",
        "    B,N,H,W = preds.shape # Batch size, num outputs, height, width\n",
        "    resize_to = H*W\n",
        "    loss = 0\n",
        "    for i in range(N):         \n",
        "        I = torch.eye(H*W)\n",
        "        belief = preds[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        target = targets[:,i,:,:].reshape((B,resize_to)).cpu()\n",
        "        logits = MVN(belief.unsqueeze(1), (BMCE_NOISE*I)).log_prob(target.unsqueeze(0))  # logit size: [batch, batch]\n",
        "        loss_temp = cross_entropy(logits, torch.arange(B))     # contrastive-like loss\n",
        "        loss_temp = loss_temp * (2 * BMCE_NOISE)\n",
        "        loss += loss_temp\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "-qLcjNT-HX_E"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2CMHoaPXt_vM"
      },
      "outputs": [],
      "source": [
        "#@title Run the network for one epoch \n",
        "def _run_network(epoch, loader, train=True):\n",
        "\n",
        "    if train:\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "\n",
        "    # Iterate through batches\n",
        "    for batch_idx, targets in enumerate(loader):\n",
        "        # Get data and targets\n",
        "        data = Variable(targets['img'].cuda())\n",
        "        target_belief = Variable(targets['beliefs'].cuda())        \n",
        "        target_affinity = Variable(targets['affinities'].cuda())\n",
        "        loss = None\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Get predictions\n",
        "        output_belief, output_affinities = net(data) \n",
        "\n",
        "        # Belief maps loss\n",
        "        for l in output_belief: #output, each belief map layers. \n",
        "            if loss is None:\n",
        "                loss = ((l - target_belief) * (l-target_belief)).mean()\n",
        "                \n",
        "            else:\n",
        "                loss_tmp = ((l - target_belief) * (l-target_belief)).mean()\n",
        "                loss += loss_tmp\n",
        "        # Get balanced mce loss for belief maps\n",
        "        loss += get_bmce_loss(output_belief, target_belief)\n",
        "\n",
        "        # Affinities loss\n",
        "        for l in output_affinities: #output, each belief map layers. \n",
        "            loss_tmp = ((l - target_affinity) * (l-target_affinity)).mean()\n",
        "            loss += loss_tmp \n",
        "        # Get balanced mce loss for affinity\n",
        "        loss += get_bmce_loss(output_affinities, target_affinity)\n",
        "\n",
        "        # Update weights\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(parameters, max_norm=MAX_NORM, norm_type=NORM_TYPE)\n",
        "            optimizer.step()\n",
        "\n",
        "        # Determine file to write loss into \n",
        "        if train:\n",
        "            namefile = '/loss_train.csv'\n",
        "        else:\n",
        "            namefile = '/loss_test.csv'\n",
        "        # Write to files\n",
        "        with open (OUT_FLDR+namefile,'a') as file:\n",
        "            s = '{}, {},{:.15f}\\n'.format(\n",
        "                epoch,batch_idx,loss.data.item()) \n",
        "            file.write(s)\n",
        "\n",
        "        # Print results\n",
        "        if train:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n",
        "        else:\n",
        "            if batch_idx % LOG_INTERVAL == 0:\n",
        "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.15f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(loader.dataset),\n",
        "                    100. * batch_idx / len(loader), loss.data.item()))\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSl6eqdbvTFN",
        "outputId": "201fa42b-e334-4365-e4de-741e1dcd94c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1220 (0%)]\tLoss: 26.079916000366211\n",
            "end: 04:58:58.268436\n"
          ]
        }
      ],
      "source": [
        "#@title Run training over all epochs \n",
        "EPOCHS = 1\n",
        "\n",
        "print (\"Start:\" , datetime.datetime.now().time())\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    if not trainingdata is None:\n",
        "        _run_network(epoch,trainingdata)\n",
        "\n",
        "    if not DATA_PATH_TEST == \"\":\n",
        "        _run_network(epoch,testingdata,train = False)\n",
        "        if  DATA_PATH == \"\":\n",
        "            break # lets get out of this if we are only testing\n",
        "    try:\n",
        "        torch.save(net.state_dict(), '{}/net_{}_{}.pth'.format(OUT_FLDR, NAME_FILE, epoch))\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    scheduler.step()\n",
        "\n",
        "print (\"End:\" , datetime.datetime.now().time())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}